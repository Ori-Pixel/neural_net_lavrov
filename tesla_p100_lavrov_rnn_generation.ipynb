{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "B3go-T9PLlly",
    "outputId": "f1353a02-1186-4b44-d00a-a9676e3894a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is 27019858 characters long\n",
      "Text is 4260425 words long\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#get current working directory\n",
    "path = os.getcwd()\n",
    "#path to lavrov json file\n",
    "text = open('/lavrov.json', 'rb').read().decode(encoding='utf-8')\n",
    "print(\"Text is {} characters long\".format(len(text)))\n",
    "#split full json file into a basic number analaysis\n",
    "words = [w for w in text.split(' ') if w.strip() != '' or w == '\\n']\n",
    "print(\"Text is {} words long\".format(len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PdLYpNdqLwy6",
    "outputId": "648b6579-bdd8-4349-bf72-eaf6d1ac06f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 4074027\n",
      "Unique Tokens: 34640\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import string\n",
    "articles_string = ''\n",
    "with open(\"/lavrov.json\") as f:\n",
    "    jsonString = f.read()\n",
    "    jsonData = json.loads(jsonString)\n",
    "    for i in range(0, len(jsonData)):\n",
    "      cleaned_articletext= jsonData[i]['ArticleText'].replace('Toggle navigation      / / Asset Publisher' ,'')\n",
    "      cleaned_articletext = cleaned_articletext.replace('Advanced settings         Switcher      6 Photos close','')\n",
    "      articles_string = articles_string + ' ' + cleaned_articletext\n",
    "\n",
    "def clean_doc(doc):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# make lower case\n",
    "\ttokens = [word.lower() for word in tokens]\n",
    "\treturn tokens\n",
    "\n",
    "def save_doc(lines, filename):\n",
    "\tdata = '\\n'.join(lines)\n",
    "\tfile = open(filename, 'w')\n",
    "\tfile.write(data)\n",
    "\tfile.close()\n",
    " \n",
    "tokens = clean_doc(articles_string)\n",
    "print('Total Tokens: {}'.format(len(tokens)))\n",
    "print('Unique Tokens: {}'.format(len(set(tokens))))\n",
    "\n",
    "\n",
    "# save sequences to file\n",
    "#out_filename = 'lavrov_sequences.txt'\n",
    "#save_doc(sequences, out_filename)\n",
    "\n",
    "#load file from save\n",
    "#lavrov_sequences = open('/lavrov_sequences.txt', 'rb').read().decode(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "x-GgiasNLy4C",
    "outputId": "d6c3cb48-3826-43e4-ff3f-b29a18ab58e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'  18 November 2019 2364-18-11-2019 Mr Makei, Mr Rapota, Colleagues, friends, Welcome to a joint meet'\n",
      " mapped to integers:\n",
      " [ 0  0 17 24  0 46 77 84 67 75 64 67 80  0 18 16 17 25  0 18 19 22 20 13\n",
      " 17 24 13 17 17 13 18 16 17 25  0 45 80  0 45 63 73 67 71 12  0 45 80  0\n",
      " 50 63 78 77 82 63 12  0 35 77 74 74 67 63 69 83 67 81 12  0 68 80 71 67\n",
      " 76 66 81 12  0 55 67 74 65 77 75 67  0 82 77  0 63  0 72 77 71 76 82  0\n",
      " 75 67 67 82]\n",
      "(25515589,) (21615872,) (3899717,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "text = articles_string\n",
    "#Map unique characters to indices\n",
    "vocab = sorted(set(text))\n",
    "char2int = {c:i for i, c in enumerate(vocab)}\n",
    "int2char = np.array(vocab)\n",
    "\n",
    "\n",
    "text_as_int = np.array([char2int[ch] for ch in text], dtype=np.int32)\n",
    "print ('{}\\n mapped to integers:\\n {}'.format(repr(text[:100]), text_as_int[:100]))\n",
    "tr_text = text_as_int[:21615872] #text separated for training, divisible by the batch size (64)\n",
    "val_text = text_as_int[21615872:] #text separated for validation\n",
    "\n",
    "print(text_as_int.shape, tr_text.shape, val_text.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ohyz3R-nMKRo"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "buffer_size = 10000\n",
    "embedding_dim = 256\n",
    "epochs = 50\n",
    "seq_length = 200\n",
    "examples_per_epoch = len(text)//seq_length\n",
    "#lr = 0.001 #will use default for Adam optimizer\n",
    "rnn_units = 1024\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "NgT-aun2MMHz",
    "outputId": "758fb870-3667-48af-9d23-acb0c771e86a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: (), types: tf.int32> <TensorSliceDataset shapes: (), types: tf.int32>\n",
      "<BatchDataset shapes: (201,), types: tf.int32> <BatchDataset shapes: (201,), types: tf.int32>\n",
      "'  18 November 2019 2364-18-11-2019 Mr Makei, Mr Rapota, Colleagues, friends, Welcome to a joint meeting of the collegiums of the foreign ministries of Russia and the Republic of Belarus. This meeting i'\n",
      "tf.Tensor(\n",
      "[ 0  0 17 24  0 46 77 84 67 75 64 67 80  0 18 16 17 25  0 18 19 22 20 13\n",
      " 17 24 13 17 17 13 18 16 17 25  0 45 80  0 45 63 73 67 71 12  0 45 80  0\n",
      " 50 63 78 77 82 63 12  0 35 77 74 74 67 63 69 83 67 81 12  0 68 80 71 67\n",
      " 76 66 81 12  0 55 67 74 65 77 75 67  0 82 77  0 63  0 72 77 71 76 82  0\n",
      " 75 67 67 82 71 76 69  0 77 68  0 82 70 67  0 65 77 74 74 67 69 71 83 75\n",
      " 81  0 77 68  0 82 70 67  0 68 77 80 67 71 69 76  0 75 71 76 71 81 82 80\n",
      " 71 67 81  0 77 68  0 50 83 81 81 71 63  0 63 76 66  0 82 70 67  0 50 67\n",
      " 78 83 64 74 71 65  0 77 68  0 34 67 74 63 80 83 81 14  0 52 70 71 81  0\n",
      " 75 67 67 82 71 76 69  0 71], shape=(201,), dtype=int32)\n",
      "' other levels.  Question :   The independence of Kosovo will be proclaimed on February 17.  What is your country going to do after the 17th of February?  How do you assess the political crisis in Serbi'\n",
      "tf.Tensor(\n",
      "[ 0 77 82 70 67 80  0 74 67 84 67 74 81 14  0  0 49 83 67 81 82 71 77 76\n",
      "  0 26  0  0  0 52 70 67  0 71 76 66 67 78 67 76 66 67 76 65 67  0 77 68\n",
      "  0 43 77 81 77 84 77  0 85 71 74 74  0 64 67  0 78 80 77 65 74 63 71 75\n",
      " 67 66  0 77 76  0 38 67 64 80 83 63 80 87  0 17 23 14  0  0 55 70 63 82\n",
      "  0 71 81  0 87 77 83 80  0 65 77 83 76 82 80 87  0 69 77 71 76 69  0 82\n",
      " 77  0 66 77  0 63 68 82 67 80  0 82 70 67  0 17 23 82 70  0 77 68  0 38\n",
      " 67 64 80 83 63 80 87 31  0  0 40 77 85  0 66 77  0 87 77 83  0 63 81 81\n",
      " 67 81 81  0 82 70 67  0 78 77 74 71 82 71 65 63 74  0 65 80 71 81 71 81\n",
      "  0 71 76  0 51 67 80 64 71], shape=(201,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tr_char_dataset = tf.data.Dataset.from_tensor_slices(tr_text)\n",
    "val_char_dataset = tf.data.Dataset.from_tensor_slices(val_text)\n",
    "print(tr_char_dataset, val_char_dataset)\n",
    "tr_sequences = tr_char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "val_sequences = val_char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "print(tr_sequences, val_sequences)\n",
    "\n",
    "for item in tr_sequences.take(1):\n",
    "    print(repr(''.join(int2char[item.numpy()])))\n",
    "    print(item)\n",
    "for item in val_sequences.take(1):\n",
    "    print(repr(''.join(int2char[item.numpy()])))\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "fTmlscUFMN4V",
    "outputId": "56d90e03-231b-44ef-be6a-bd976bbf604b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((128, 200), (128, 200)), types: (tf.int32, tf.int32)> <BatchDataset shapes: ((128, 200), (128, 200)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "tr_dataset = tr_sequences.map(split_input_target).shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
    "val_dataset = val_sequences.map(split_input_target).shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
    "print(tr_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5bdxE3cuMTHd"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.LSTM(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dropout(0.2), \n",
    "        tf.keras.layers.LSTM(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_6bIAAgvMVpT"
   },
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "    vocab_size = len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "v1FY0T3-MXET",
    "outputId": "7082b367-ccc0-4ffb-e382-aa45d559f55e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 200, 89) respectively: batch_size, sequence_length, vocab_size\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in tr_dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"respectively: batch_size, sequence_length, vocab_size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "LCnec2jQMYeN",
    "outputId": "e097f8ea-ad6e-43fd-8e33-9dd004198231"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " 's. These are steps in the right direction. We will try and help them become a reality. As soon as it becomes clear that the decisions of the October 2016 summit in Berlin are fulfilled, I think that a'\n",
      "\n",
      "Predictions: \n",
      " \"cEeesQ9%2C'lJJg2FWNMhav<4nmEvY>7b8s)@'o9]0*PBtd*?Djz<D@G9n;#l>; <DU-+03tC`dGR/r<MAp%UAFUf vsJ DWG9Z.lIYTe!4o&$di3vdUjaBd8zJ&FKx.xpYv_Hx?n/CEg;<@h#hhdqef(X?_fr6M@<A%h5rqi-dv<E]d(6bT,YLU6)vrUEjeJ4JetswC\"\n"
     ]
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "print(\"Input: \\n\", repr(\"\".join(int2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Predictions: \\n\", repr(\"\".join(int2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "9jY1kmPDMail",
    "outputId": "0c79af98-10bf-46fa-d6e8-16faaf1129c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (128, 200, 89)  # (batch_size, sequence_length, vocab_size)\n",
      "Loss:       4.488257\n",
      "Accuracy:       0.020117188\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "def accuracy(labels, logits):\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(labels, logits)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "example_batch_acc  = accuracy(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Loss:      \", example_batch_loss.numpy().mean())\n",
    "print(\"Accuracy:      \", example_batch_acc.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j0VO_wlbMfab"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam() \n",
    "#deafults = learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False\n",
    "model.compile(optimizer=optimizer, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ctp-FkNWMhM-"
   },
   "outputs": [],
   "source": [
    "patience = 2\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sirzAGnDMjNZ"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "checkpoint_dir = './checkpoints'+ datetime.datetime.now().strftime(\"_%Y.%m.%d-%H:%M:%S\")\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 938
    },
    "colab_type": "code",
    "id": "KEM-psNfMlku",
    "outputId": "06607cea-b3ac-41df-ae57-389be5958979"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "840/840 [==============================] - 344s 409ms/step - loss: 1.5631 - val_loss: 0.9619\n",
      "Epoch 2/50\n",
      "840/840 [==============================] - 344s 410ms/step - loss: 0.9849 - val_loss: 0.8770\n",
      "Epoch 3/50\n",
      "840/840 [==============================] - 345s 411ms/step - loss: 0.9130 - val_loss: 0.8433\n",
      "Epoch 4/50\n",
      "840/840 [==============================] - 346s 412ms/step - loss: 0.8764 - val_loss: 0.8242\n",
      "Epoch 5/50\n",
      "840/840 [==============================] - 348s 414ms/step - loss: 0.8528 - val_loss: 0.8115\n",
      "Epoch 6/50\n",
      "840/840 [==============================] - 344s 410ms/step - loss: 0.8358 - val_loss: 0.8013\n",
      "Epoch 7/50\n",
      "840/840 [==============================] - 349s 415ms/step - loss: 0.8231 - val_loss: 0.7939\n",
      "Epoch 8/50\n",
      "840/840 [==============================] - 346s 411ms/step - loss: 0.8131 - val_loss: 0.7910\n",
      "Epoch 9/50\n",
      "840/840 [==============================] - 347s 413ms/step - loss: 0.8050 - val_loss: 0.7878\n",
      "Epoch 10/50\n",
      "840/840 [==============================] - 346s 412ms/step - loss: 0.7982 - val_loss: 0.7829\n",
      "Epoch 11/50\n",
      "840/840 [==============================] - 347s 413ms/step - loss: 0.7928 - val_loss: 0.7820\n",
      "Epoch 12/50\n",
      "840/840 [==============================] - 348s 415ms/step - loss: 0.7881 - val_loss: 0.7798\n",
      "Epoch 13/50\n",
      "840/840 [==============================] - 345s 410ms/step - loss: 0.7841 - val_loss: 0.7780\n",
      "Epoch 14/50\n",
      "840/840 [==============================] - 349s 415ms/step - loss: 0.7808 - val_loss: 0.7758\n",
      "Epoch 15/50\n",
      "840/840 [==============================] - 345s 410ms/step - loss: 0.7780 - val_loss: 0.7753\n",
      "Epoch 16/50\n",
      "840/840 [==============================] - 347s 413ms/step - loss: 0.7756 - val_loss: 0.7759\n",
      "Epoch 17/50\n",
      "840/840 [==============================] - 347s 413ms/step - loss: 0.7734 - val_loss: 0.7747\n",
      "Epoch 18/50\n",
      "840/840 [==============================] - 347s 413ms/step - loss: 0.7717 - val_loss: 0.7728\n",
      "Epoch 19/50\n",
      "840/840 [==============================] - 348s 415ms/step - loss: 0.7699 - val_loss: 0.7730\n",
      "Epoch 20/50\n",
      "840/840 [==============================] - 345s 411ms/step - loss: 0.7687 - val_loss: 0.7721\n",
      "Epoch 21/50\n",
      "840/840 [==============================] - 349s 415ms/step - loss: 0.7674 - val_loss: 0.7724\n",
      "Epoch 22/50\n",
      "840/840 [==============================] - 345s 411ms/step - loss: 0.7664 - val_loss: 0.7719\n",
      "Epoch 23/50\n",
      "840/840 [==============================] - 347s 413ms/step - loss: 0.7655 - val_loss: 0.7716\n",
      "Epoch 24/50\n",
      "840/840 [==============================] - 346s 412ms/step - loss: 0.7646 - val_loss: 0.7705\n",
      "Epoch 25/50\n",
      "840/840 [==============================] - 347s 413ms/step - loss: 0.7640 - val_loss: 0.7711\n",
      "Epoch 26/50\n",
      "840/840 [==============================] - 348s 414ms/step - loss: 0.7635 - val_loss: 0.7713\n",
      "Training stopped as there was no improvement after 2 epochs\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(tr_dataset, epochs=epochs, callbacks=[checkpoint_callback, early_stop] , validation_data=val_dataset)\n",
    "print (\"Training stopped as there was no improvement after {} epochs\".format(patience))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "colab_type": "code",
    "id": "z8eBI8tpMnL4",
    "outputId": "c3120453-1c55-4d8f-9ab0-d0031c89c4ee"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-6c7061b63642>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'g'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rx'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#use if have val data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x648 with 0 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.plot(history.history['loss'], 'g')\n",
    "plt.plot(history.history['val_loss'], 'rx') #use if have val data\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train'], loc='upper right')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right') #use if have val date\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "cf23MP4p0s2B",
    "outputId": "ceb0ed8c-6f64-4534-d425-7bd5df48e23c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with seed: \"In\"\n",
      "Independent article so far, as the commission, is still in the process of solving the problem of e of Russian culture.  We paid special attention to the fordation of a new document where the decision made by US group of states in personnel that could make it possible to create a shift to the synthesis of the conflicts in Iraq, where Abkhazia and South Ossetia cannot be notified. We presume that we want to give the goal of preventing a provocation and help curb this in Libya now. I hope all what is happening in Abkhazia and South Ossetia.  We agreed to have enough organized state auspices.  We regret that the centre of document Development Special Tows and the choice the all of the real financial system in Europe is already a  interview his attention to this issue.  We do not understand why the conference worked as a result of your decision to commence the decisions of the UN Security Council, which centralizes it are being clearly decided to come.   At the same time European Union positi\n"
     ]
    }
   ],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir)) \n",
    "model.build(tf.TensorShape([1, None]))\n",
    "def generate_text(model, start_string):\n",
    "    \n",
    "    print('Generating with seed: \"' + start_string + '\"')\n",
    "  \n",
    "    num_generate = 1000\n",
    "    input_eval = [char2int[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    text_generated = []\n",
    "    temperature = 1.0\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions,      num_samples=1)[-1,0].numpy()\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(int2char[predicted_id])\n",
    "    return (start_string + ''.join(text_generated))\n",
    "print(generate_text(model, start_string=\"In\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uhp8vokdRCQ-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "tesla_p100_lavrov_rnn_generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
